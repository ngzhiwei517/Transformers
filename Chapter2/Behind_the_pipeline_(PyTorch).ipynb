{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngzhiwei517/Transformers/blob/main/Chapter2/Behind_the_pipeline_(PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0djfV0ds7zBh"
      },
      "source": [
        "# Behind the pipeline (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn3cVQMJ7zBi"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2-vLR6C7zBj"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you call the tokenizer, it does 3 things:\n",
        "\n",
        "Tokenizes the text:\n",
        "Breaks sentences into smaller pieces called tokens (words, subwords, or punctuation).\n",
        "\n",
        "Maps tokens to numbers:\n",
        "Every token has a unique ID from the model‚Äôs vocabulary.\n",
        "Example: \"HuggingFace\" ‚Üí [12345]\n",
        "\n",
        "Adds special tokens (depending on the model):\n",
        "For example, [CLS] and [SEP] tokens for BERT.\n",
        "\n"
      ],
      "metadata": {
        "id": "lvLskHQFFUtK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRbt2ipL7zBk"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tells Hugging Face:\n",
        "\n",
        "\"Give me the tokenizer that was used when this model was trained\"\n",
        "\n",
        "The tokenizer is downloaded once and cached"
      ],
      "metadata": {
        "id": "LKyDrojrFaPZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-box2BRo7zBl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ return_tensors=\"pt\"\n",
        "üëâ Returns the output as PyTorch tensors, not plain Python lists or NumPy arrays.\n",
        "\n",
        "Other options:\n",
        "\n",
        "\"tf\" ‚Üí TensorFlow\n",
        "\n",
        "\"np\" ‚Üí NumPy arrays"
      ],
      "metadata": {
        "id": "qJDqkLObFwV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üîπ padding=True**\n",
        "\n",
        "üëâ Ensures all sentences are the same length by adding special [PAD] tokens.\n",
        "\n",
        "Yes, the longest sentence in the batch becomes the reference length.\n",
        "\n",
        "üîç Step 1: Tokenize both sentences\n",
        "\"I love AI\" ‚Üí [101, 1045, 2293, 1034, 102] ‚Üí length = 5\n",
        "\n",
        "\"I hate math!\" ‚Üí [101, 1045, 5223, 4667, 999, 102] ‚Üí length = 6\n",
        "\n",
        "So, the longest one is length 6.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "I0cTSaaLGtOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîß Step 2: Padding the shorter sentence**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "To make both sequences the same length, the shorter one (length = 5) needs to be padded to length = 6.\n",
        "\n",
        "So the first sentence becomes:\n",
        "[101, 1045, 2293, 1034, 102, 0]\n",
        "\n"
      ],
      "metadata": {
        "id": "d6jhymgXHHKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **truncation=True**\n",
        "\n",
        "means it will cut off extra tokens if your input is too long for the model to handle.\n",
        "\n",
        "Most transformer models (like BERT, DistilBERT, GPT-2) have a maximum input length, usually 512 tokens.\n",
        "\n",
        "If your sentence or paragraph is longer than that ‚Äî for example, 600 tokens ‚Äî it will cut off the extra tokens beyond 512.\n",
        "\n",
        "üí¨ Example:\n",
        "Imagine this input:\n",
        "\n",
        "\"The movie was slow at first, but the ending was absolutely amazing. One of the best twists I've ever seen!\"\n",
        "\n",
        "If the tokenizer cuts it off before \"the ending was absolutely amazing...\", then the model might wrongly predict that the sentiment is negative ‚Äî because it only sees the \"slow at first\" part. üò¨\n",
        "\n"
      ],
      "metadata": {
        "id": "h7hs5gppHm_e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io5H1lja7zBl"
      },
      "outputs": [],
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\") # 1 = token used\n",
        "print(inputs) # 1 = token used, 0 = padding ignored"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key points:\n",
        "\n",
        "input_ids: numbers representing each word/token, plus special tokens like [101] (start) and [102] (end).\n",
        "\n",
        "Padding tokens (0) added to the shorter sentence \"I hate this so much!\" to match the length of the longer one.\n",
        "\n",
        "attention_mask tells the model which tokens are real (1) and which are padding (0) so it ignores padding during processing.\n",
        "\n"
      ],
      "metadata": {
        "id": "7ezk2XUiMxIi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abQdhDUX7zBl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet loads a pre-trained DistilBERT model without a classification head.\n",
        "\n",
        "This imports the AutoModel class, which is used to load just the base transformer model without any task-specific heads.\n",
        "\n",
        "No Head: Note that this loads just the base model, not the version with a classification head."
      ],
      "metadata": {
        "id": "s5efIZASTGxh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZd1K2Y77zBm"
      },
      "outputs": [],
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the input is a single sentence with 8 tokens, the shape might be:\n",
        "\n",
        "(1, 8, 768)\n",
        "\n",
        "This means:\n",
        "\n",
        "1 sequence\n",
        "\n",
        "8 tokens\n",
        "\n",
        "768 hidden state dimensions"
      ],
      "metadata": {
        "id": "KQxySGqrUv4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ If you want a complete model for a task, you should use:"
      ],
      "metadata": {
        "id": "ErSqwwgKVXY3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38LnDO797zBm"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives you the base + task-specific head ‚Üí now it can predict sentiment or any other task it's trained for."
      ],
      "metadata": {
        "id": "2m0oSNyWVZQB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RafOJcGY7zBn"
      },
      "outputs": [],
      "source": [
        "print(outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.Size([2, 2])\n",
        "\n",
        "Dimension\tMeaning\n",
        "2 (first)\tYou gave 2 input sentences (a batch of size 2) ‚úÖ\n",
        "\n",
        "2 (second)\tThe model predicts 2 classes (positive or negative sentiment) ‚úÖ\n"
      ],
      "metadata": {
        "id": "zx5MaICBVkJx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smxzV8xc7zBn"
      },
      "outputs": [],
      "source": [
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÅ For each input sentence, it outputs 2 numbers = the logits (scores) for the two classes."
      ],
      "metadata": {
        "id": "dFgP2-JAWGtL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "represents the logits (raw scores) for two sentences:\n",
        "\n",
        "First sentence: scores for class 0 (negative) = -1.5607 and class 1 (positive) = 1.6123\n",
        "\n",
        "Second sentence: scores for class 0 = 4.1692 and class 1 = -3.3464\n",
        "\n"
      ],
      "metadata": {
        "id": "4UUWn4JrWSYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***But these are logits, not probabilities yet! ü§ì***"
      ],
      "metadata": {
        "id": "XaL32xvVWU7r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Lea7d8R7zBn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does this mean?\n",
        "\n",
        "For Sentence 1, the model is 95.98% sure it‚Äôs positive sentiment.\n",
        "\n",
        "For Sentence 2, the model is 99.95% sure it‚Äôs negative sentiment."
      ],
      "metadata": {
        "id": "mQDXeUIqaG0R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQSO_mNj7zBn"
      },
      "outputs": [],
      "source": [
        "model.config.id2label"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}