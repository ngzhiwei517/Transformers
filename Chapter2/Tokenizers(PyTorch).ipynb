{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngzhiwei517/Transformers/blob/main/Chapter2/Tokenizers(PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnZmaD-0F2Ew"
      },
      "source": [
        "# Tokenizers (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uORfnqGF2Ez"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuSM9YZdF2E1"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nEBiB8hF2E3"
      },
      "outputs": [],
      "source": [
        "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox4IUbPGF2E6"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDyxIlaeF2E7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Use AutoTokenizer?\n",
        "\n",
        "Flexibility: It automatically detects the correct tokenizer class based on the model name you provide, like \"bert-base-cased\".\n",
        "\n",
        "Ease of Use: You donâ€™t have to remember the specific tokenizer class for each model, reducing the chance of making mistakes"
      ],
      "metadata": {
        "id": "e3Clh8DVYpQv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cpe2ALI1F2E7"
      },
      "outputs": [],
      "source": [
        "tokenizer(\"Using a Transformer network is simple\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **input_ids ðŸ†”**\n",
        "\n",
        "These are the token IDs corresponding to the subword tokens.\n",
        "\n",
        "The special tokens 101 and 102 are [CLS] (start of sentence) and [SEP] (end of sentence), respectively, for BERT.\n",
        "\n",
        "\n",
        "# **attention_mask ðŸ‘ï¸**\n",
        "\n",
        "This tells the model which tokens to pay attention to and which to ignore (padding).\n",
        "\n",
        "1 means the token is real (not padding), while 0 means itâ€™s padding.\n",
        "\n",
        "Since you have no padding here, itâ€™s all 1s."
      ],
      "metadata": {
        "id": "RnifcKQEZvWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What are token_type_ids Used For?**\n",
        "\n",
        "Single Sentence: All token_type_ids are 0.\n",
        "\n",
        "Sentence Pairs: 0 for the first sentence and 1 for the second sentence."
      ],
      "metadata": {
        "id": "D9_Y48mfa8bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Transformers are amazing!\", \"They power modern NLP models.\")"
      ],
      "metadata": {
        "id": "lhgBjRO_bBDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Breaking This Down:\n",
        "\n",
        "First Sentence: [101, 16297, 1132, 12245, 106, 102]\n",
        "\n",
        "Token type 0 for each token.\n",
        "\n",
        "Second Sentence: [1387, 1700, 1292, 1565, 10883, 1647, 119, 102]\n",
        "\n",
        "Token type 1 for each token.\n",
        "\n",
        "Why Do We Use This?\n",
        "It helps the model distinguish between the two sentences.\n",
        "\n",
        "Crucial for tasks like question answering and next sentence prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "N5DY5zLXbHga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Example`\n",
        "\n",
        "\n",
        "Why is token_type_ids Critical Here?\n",
        "It helps the model distinguish the question from the context, even though they are in the same input sequence.\n",
        "\n",
        "This is crucial for the BERT model to understand where the question ends and the context begins, ensuring it can locate the correct answer span.\n",
        "\n"
      ],
      "metadata": {
        "id": "sQGmo3R4blSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "context = \"Transformers are neural networks used for NLP.\"\n",
        "question = \"What are transformers used for?\"\n",
        "\n",
        "tokenizer(question, context)\n",
        "\n"
      ],
      "metadata": {
        "id": "5_xqBl9ybhQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrIQpsLGF2E8"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(\"directory_on_my_computer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU_wSKTkF2E9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"Using a Transformer network is simple\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tokenizer is a subword tokenizer: it splits the words until it obtains tokens that can be represented by its vocabulary\n",
        "\n",
        "\n",
        "Why Split \"Transformer\" into \"transform\" and \"##er\"?\n",
        "> The BERT tokenizer uses WordPiece tokenization, which tries to keep the vocabulary small while still covering most words.\n",
        "\n",
        "Hereâ€™s a simplified view of what might be in vocab.txt:\n",
        "\n",
        "\n",
        "transform  \n",
        "er  \n",
        "transformation  \n",
        "network  \n",
        "is  \n",
        "simple  \n",
        "...\n",
        "\n",
        "Notice that \"transformer\" is not included, but its components \"transform\" and \"##er\" are.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "ðŸ§  Why This is Smart\n",
        "\n",
        "This approach handles both common and rare words efficiently.\n",
        "\n",
        "It can represent new words like \"transforming\" (transform + ##ing) or \"transformers\" (transform + ##ers) without needing new vocabulary.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z5FNy0N7s2Kf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_s9sYQDF2E-"
      },
      "outputs": [],
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHlkHoRZF2E-"
      },
      "outputs": [],
      "source": [
        "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
        "print(decoded_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. tokenizer.tokenize() (Tokenization Only)**\n",
        "\n",
        "\n",
        "\n",
        "*   Does not add special tokens like [CLS] or [SEP].\n",
        "\n",
        "\n",
        "*   Returns a list of strings.\n",
        "\n",
        "\n",
        "*  No attention mask or token type IDs are generated.\n",
        "\n",
        "\n",
        "\n",
        "# **2. tokenizer() (Full Encoding)**\n",
        "\n",
        "\n",
        "Includes tokenization plus:\n",
        "\n",
        "* [CLS] (start) and [SEP] (end) tokens.\n",
        "\n",
        "* Converts tokens to IDs using the modelâ€™s vocabulary.\n",
        "\n",
        "\n",
        "\n",
        "*   Adds token type IDs and attention mask.\n",
        "\n",
        "\n",
        "*   Ready for Model Input.\n",
        "\n",
        "\n",
        "* tokenizer.tokenize() â†’ Shows the internal tokenization process (subword splitting).\n",
        "\n",
        "* tokenizer() â†’ Directly prepares the full input for the model,\n",
        "\n"
      ],
      "metadata": {
        "id": "DaYg_uoer36R"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tokenizers (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}