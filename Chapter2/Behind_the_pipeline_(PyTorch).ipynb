{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngzhiwei517/Transformers/blob/main/Chapter2/Behind_the_pipeline_(PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0djfV0ds7zBh"
      },
      "source": [
        "# Behind the pipeline (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qn3cVQMJ7zBi"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2-vLR6C7zBj"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you call the tokenizer, it does 3 things:\n",
        "\n",
        "Tokenizes the text:\n",
        "Breaks sentences into smaller pieces called tokens (words, subwords, or punctuation).\n",
        "\n",
        "Maps tokens to numbers:\n",
        "Every token has a unique ID from the model‚Äôs vocabulary.\n",
        "Example: \"HuggingFace\" ‚Üí [12345]\n",
        "\n",
        "Adds special tokens (depending on the model):\n",
        "For example, [CLS] and [SEP] tokens for BERT.\n",
        "\n"
      ],
      "metadata": {
        "id": "lvLskHQFFUtK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRbt2ipL7zBk"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tells Hugging Face:\n",
        "\n",
        "\"Give me the tokenizer that was used when this model was trained\"\n",
        "\n",
        "The tokenizer is downloaded once and cached"
      ],
      "metadata": {
        "id": "LKyDrojrFaPZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-box2BRo7zBl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ return_tensors=\"pt\"\n",
        "üëâ Returns the output as PyTorch tensors, not plain Python lists or NumPy arrays.\n",
        "\n",
        "Other options:\n",
        "\n",
        "\"tf\" ‚Üí TensorFlow\n",
        "\n",
        "\"np\" ‚Üí NumPy arrays"
      ],
      "metadata": {
        "id": "qJDqkLObFwV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üîπ padding=True**\n",
        "\n",
        "üëâ Ensures all sentences are the same length by adding special [PAD] tokens.\n",
        "\n",
        "Yes, the longest sentence in the batch becomes the reference length.\n",
        "\n",
        "üîç Step 1: Tokenize both sentences\n",
        "\"I love AI\" ‚Üí [101, 1045, 2293, 1034, 102] ‚Üí length = 5\n",
        "\n",
        "\"I hate math!\" ‚Üí [101, 1045, 5223, 4667, 999, 102] ‚Üí length = 6\n",
        "\n",
        "So, the longest one is length 6.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "I0cTSaaLGtOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîß Step 2: Padding the shorter sentence**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "To make both sequences the same length, the shorter one (length = 5) needs to be padded to length = 6.\n",
        "\n",
        "So the first sentence becomes:\n",
        "[101, 1045, 2293, 1034, 102, 0]\n",
        "\n"
      ],
      "metadata": {
        "id": "d6jhymgXHHKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **truncation=True**\n",
        "\n",
        "means it will cut off extra tokens if your input is too long for the model to handle.\n",
        "\n",
        "Most transformer models (like BERT, DistilBERT, GPT-2) have a maximum input length, usually 512 tokens.\n",
        "\n",
        "If your sentence or paragraph is longer than that ‚Äî for example, 600 tokens ‚Äî it will cut off the extra tokens beyond 512.\n",
        "\n",
        "üí¨ Example:\n",
        "Imagine this input:\n",
        "\n",
        "\"The movie was slow at first, but the ending was absolutely amazing. One of the best twists I've ever seen!\"\n",
        "\n",
        "If the tokenizer cuts it off before \"the ending was absolutely amazing...\", then the model might wrongly predict that the sentiment is negative ‚Äî because it only sees the \"slow at first\" part. üò¨\n",
        "\n"
      ],
      "metadata": {
        "id": "h7hs5gppHm_e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io5H1lja7zBl"
      },
      "outputs": [],
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abQdhDUX7zBl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZd1K2Y77zBm"
      },
      "outputs": [],
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38LnDO797zBm"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RafOJcGY7zBn"
      },
      "outputs": [],
      "source": [
        "print(outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smxzV8xc7zBn"
      },
      "outputs": [],
      "source": [
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Lea7d8R7zBn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQSO_mNj7zBn"
      },
      "outputs": [],
      "source": [
        "model.config.id2label"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}